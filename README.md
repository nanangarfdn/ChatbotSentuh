# Sentuh Tanahku Chatbot API

Enhanced conversational AI API with Indonesian NLP, vector embeddings, and PostgreSQL database integration.

## ğŸ—ï¸ Project Structure (Refactored)

```
ChatbotSentuh/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ api/                      # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py              # Main FastAPI app
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”‚   â””â”€â”€ endpoints/           # API endpoints (planned)
â”‚   â”œâ”€â”€ core/                    # Core business logic
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration & feature detection
â”‚   â”‚   â”œâ”€â”€ nlp/                 # Indonesian NLP processing
â”‚   â”‚   â”‚   â”œâ”€â”€ processor.py     # Text processing & similarity
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation.py  # Conversation handling
â”‚   â”‚   â”‚   â””â”€â”€ confidence.py    # Confidence evaluation
â”‚   â”‚   â””â”€â”€ embeddings/          # Vector embeddings
â”‚   â”‚       â””â”€â”€ vector_store.py  # ChromaDB vector storage
â”‚   â”œâ”€â”€ database/                # Database layer
â”‚   â”‚   â””â”€â”€ connection.py        # PostgreSQL connection & queries
â”‚   â”œâ”€â”€ services/                # Business services
â”‚   â”‚   â”œâ”€â”€ query_processor.py   # Main query processing
â”‚   â”‚   â””â”€â”€ response_generator.py # Response generation
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â””â”€â”€ cache.py            # LRU cache implementation
â”œâ”€â”€ tests/                       # Test files
â”‚   â””â”€â”€ test_database.py        # Database tests
â”œâ”€â”€ data/                        # Data files
â”‚   â”œâ”€â”€ csv/                     # CSV data files
â”‚   â””â”€â”€ embeddings/              # Vector embeddings storage
â”‚       â””â”€â”€ chroma_db/          # ChromaDB files
â”œâ”€â”€ docker/                      # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ config/                      # Configuration files
â”‚   â””â”€â”€ credentials.json
â”œâ”€â”€ app.py                       # Development entry point (with reload)
â”œâ”€â”€ main.py                      # Production entry point (refactored)
â””â”€â”€ requirements.txt
```

## ğŸš€ Features

- **Multi-layer Query Processing**: Vector embeddings + NLP similarity + conversation AI
- **Indonesian NLP**: Advanced text processing with Sastrawi stemming & stopword removal
- **Vector Embeddings**: ChromaDB with SentenceTransformers for semantic search
- **PostgreSQL Integration**: Persistent storage with optimized bulk operations
- **Performance Optimizations**: LRU caching, background processing, parallel execution
- **RESTful API**: FastAPI with streaming support and comprehensive endpoints

## ğŸ› ï¸ Installation

1. **Clone & Setup**:
   ```bash
   git clone <repository>
   cd ChatbotSentuh
   pip install -r requirements.txt
   ```

2. **Environment Configuration**:
   ```bash
   cp config/.env.example .env
   # Edit .env with your PostgreSQL credentials
   ```

3. **Database Setup**:
   ```bash
   # Create PostgreSQL database and faqs table
   # Update .env with database credentials
   ```

## ğŸƒâ€â™‚ï¸ Running the Application

### Development Mode (Recommended):
```bash
# Main entry point (with auto-reload)
python app.py

# Alternative: Direct uvicorn
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode:
```bash
# Production entry point (no reload) - using app.py but without reload
python -c "
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
import uvicorn
uvicorn.run('src.api.main:app', host='0.0.0.0', port=8000, reload=False)
"

# Or: Direct module execution
python -m src.api.main
```

### Docker:
```bash
# Simple run (builds automatically)
cd docker/
docker-compose up --build -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f sentuh-tanahku-api

# Stop
docker-compose down
```

## ğŸ“¡ API Endpoints

### Core Endpoints:
- `GET /` - API information
- `GET /health` - Health check
- `POST /query` - Main query endpoint
- `POST /chat-stream` - Streaming chat endpoint

### Data Management:
- `POST /sync` - Sync database data
- `POST /add-faq` - Add single FAQ
- `POST /upload-csv` - Bulk upload CSV

### System:
- `GET /data-info` - Data statistics
- `GET /cache-stats` - Cache performance
- `POST /clear-cache` - Clear caches

## ğŸ§ª Testing

### Local Testing:
```bash
# Test database connection
python tests/test_database.py

# Test imports
python -c "import sys; sys.path.insert(0, 'src'); from src.api.main import app; print('API import successful')"

# Test app.py entry point
python app.py
# Should start server on http://localhost:8000
```

### Docker Testing:
```bash
# Build and test Docker container
cd docker/
docker-compose up --build

# Test API health
curl http://localhost:8000/health

# Test main endpoint
curl http://localhost:8000/

# Clean up
docker-compose down
```

## ğŸ”§ Architecture

### Processing Pipeline:
1. **Conversation Detection** - Greetings, goodbyes
2. **Vector Search** - Semantic similarity (ChromaDB)
3. **NLP Search** - Text similarity (optimized indexes)
4. **Confidence Evaluation** - Multi-factor scoring
5. **Response Generation** - Context-aware responses

### Performance Features:
- **Smart Caching**: LRU cache with selective caching based on confidence
- **Background Processing**: Non-blocking vector embeddings initialization  
- **Parallel Execution**: Async vector + NLP search
- **Optimized Indexing**: Pre-computed keyword lookup for ultra-fast search

## ğŸ“Š Monitoring

### Cache Statistics:
```bash
curl http://localhost:8000/cache-stats
```

### Health Check:
```bash
curl http://localhost:8000/health
```

## ğŸ”„ Migration Notes

This is a refactored version of the original monolithic `main.py`. Key improvements:

1. **Modular Structure**: Separated concerns into logical modules
2. **Better Imports**: Clear dependency structure
3. **Testability**: Isolated components for easier testing
4. **Maintainability**: Smaller, focused files
5. **Scalability**: Easier to extend and modify

## ğŸ› Troubleshooting

### Import Issues:
```bash
# Ensure src is in Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
```

### Database Connection:
```bash
# Test database
python tests/test_database.py
```

### Vector Embeddings:
- ChromaDB files stored in `data/embeddings/chroma_db/`
- Automatic fallback to NLP-only mode if embeddings fail

## ğŸ“ TODO

- [ ] Add comprehensive API endpoint tests
- [ ] Implement API versioning
- [ ] Add monitoring/logging service
- [ ] Create migration scripts
- [ ] Add CI/CD configuration